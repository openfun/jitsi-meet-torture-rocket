# Load test nÂ°5, 24/02/2022

## Context 

We want to test how Jitsi handles big conferences after implementing autoscaling. To enable this, we needed to deploy way more ressources be it for Jitsi or for load testing.

---

## Description of the infrastucture

The Jitsi infrastucture we are working on is deployed on Kubernetes on Scaleway. It is based on the deployment available on the [jitsi-k8s repository of OpenFUN](https://github.com/openfun/jitsi-k8s/tree/59bdc9c799db3f0decedbb4b6f870f246091d7c8). More precisely, here are the specs of the JVB nodepool on our cluster:

- 1 server
- 20-90 JVBs GP1-XS
- HPA

---

## Approach

We deployed Jitsi-Meet-Torture instances in the cloud (on multiple instances on Scaleway) to apply a high load on the infrastructure. We can therefore legitimately assume that we emulate the perfect participants in our conferences that send and receive audio and video without any client-side limit. 

We did this test with four computers launching the same number of jitsi-meet-torture instances.

We loaded a big conference of 500 participants and 126 conferences of 30 participants. 


---

## Results

All metrics were gathered with Prometheus and visualized with Grafana.

The process was to observe the autoscaling, and how it handles a great number of participants. What component breaks first and why. 

The number of participants have been tracked down to follow the evolution of metrics in terms of participants:

![Participants](resources/Participants.png)

Same goes for the number of conferences:

![Conferences](resources/Conferences.png)

### Graphs




## Interpretation of results

Video and audio quality were correct whatever the number of participants because Jitsi limits the number of open cameras and its unrealistic to imagine too many people talking at the same time.

We note that reaching 5000 participants broke Prosody. Checking the logs, we saw that Kubernetes shut it down due the lack of responce to liveness probe. The Prosody was to slow to answer. Looking at other components, the front deals with too many requests resulting in the overloading of the Prosody.
 
## Conclusion

According to the tests, we need to resolve the overloading. A solution could be to put an ingress controler to handle the requests instead of nginx.
